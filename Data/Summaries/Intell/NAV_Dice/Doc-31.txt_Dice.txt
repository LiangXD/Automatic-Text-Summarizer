Until_now , each parallel computer has been so different that users had to re-write applications every time they changed machines .While some manufacturers advocated the latter , a more popular approach has been to build computers out_of conventional processors so that many current programs and , more importantly , programming concepts , can be recycled .Parallel computing has matured so much that users and manufacturers are beginning to discuss standardisation .The Edinburgh Concurrent Supercomputer Project ( ECSP ) , under Professor David Wallace , now has the largest parallel computer in Europe , with 400 processors which can perform 400 million arithmetical calculations a second .how should a parallel computer be programmed ?At a seminar hosted by the Edinburgh Concurrent Supercomputer Project last month , researchers and vendors could finally say with confidence that supercomputing in the 1990s and beyond will be done by parallel computers .Modern science is critically dependent on high-performance computing ; studies of the world '_ s changing climate , structural engineering , and medical imaging simply could not have progressed to their present state without access to the sort of computing power that can only be provided by parallel machines .His group has been involved in parallel computing since the early 1980s ; software produced at CalTech has become a de_facto standard for the various & bquo ; hypercube & equo ; machines manufactured in the United States .Would it be possible to re-use old programs written for conventional machines , or would users have to start from scratch and throw away their existing investment in software ?The advantage of this approach is that it can be scaled up_to build computers of any size and speed .There are practical difficulties with parallel computers .What this power has meant to the scientific and industrial users involved in the ECSP was made apparent during the project '_ s second annual seminar last month .Parallel lines to the future : Britain is among the front runners as tomorrow '_ s supercomputers take shape , says Greg Wilson By GREG WILSON FROM desk-top micros to the largest mainframes , conventional computers are built around a single processor and a single store of memory .Data can not be moved from the processor to the memory and back faster than a certain speed , which limits the performance of the computer as a whole .Faced with these problems , computer scientists began in the mid 1970s to experiment with using many processors in a single machine to work in parallel on a single problem .There are fundamental limits to how fast a conventional computer can go .